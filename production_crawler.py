#!/usr/bin/env python3
"""
ç”Ÿäº§çº§ä»£ç†çˆ¬è™«æ¨¡æ¿
åŸºäºæµ‹è¯•éªŒè¯çš„ä»£ç†é…ç½®
"""

import requests
import time
import random
import json
from urllib.parse import urljoin, urlparse
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import logging

class ProductionCrawler:
    def __init__(self, delay_range=(1, 3), max_retries=3):
        """
        åˆå§‹åŒ–ç”Ÿäº§çº§çˆ¬è™«
        
        Args:
            delay_range: è¯·æ±‚é—´éš”èŒƒå›´ (æœ€å°, æœ€å¤§) ç§’
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        # ä»£ç†é…ç½® - å·²éªŒè¯å·¥ä½œæ­£å¸¸
        self.proxies = {
            'http': 'http://ravu01r1:ravu01r1@61.172.168.161:2081',
            'https': 'http://ravu01r1:ravu01r1@61.172.168.161:2081'
        }
        
        # åˆ›å»ºsession
        self.session = requests.Session()
        self.session.proxies = self.proxies
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=max_retries,
            status_forcelist=[429, 500, 502, 503, 504],
            method_whitelist=["HEAD", "GET", "OPTIONS"],
            backoff_factor=1
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # è®¾ç½®çœŸå®æµè§ˆå™¨Headers
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        
        self.delay_range = delay_range
        self.request_count = 0
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def get_page(self, url, timeout=10, **kwargs):
        """
        è·å–é¡µé¢å†…å®¹
        
        Args:
            url: ç›®æ ‡URL
            timeout: è¶…æ—¶æ—¶é—´
            **kwargs: å…¶ä»–requestså‚æ•°
            
        Returns:
            requests.Responseå¯¹è±¡æˆ–None
        """
        self.request_count += 1
        
        try:
            self.logger.info(f"è¯·æ±‚ #{self.request_count}: {url}")
            
            response = self.session.get(url, timeout=timeout, **kwargs)
            response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
            
            self.logger.info(f"æˆåŠŸ {response.status_code}: {len(response.content)} å­—èŠ‚")
            
            # æ™ºèƒ½å»¶è¿Ÿ
            self.smart_delay()
            
            return response
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
            return None
    
    def post_data(self, url, data=None, json_data=None, timeout=10, **kwargs):
        """
        å‘é€POSTè¯·æ±‚
        
        Args:
            url: ç›®æ ‡URL
            data: è¡¨å•æ•°æ®
            json_data: JSONæ•°æ®
            timeout: è¶…æ—¶æ—¶é—´
            **kwargs: å…¶ä»–requestså‚æ•°
            
        Returns:
            requests.Responseå¯¹è±¡æˆ–None
        """
        self.request_count += 1
        
        try:
            self.logger.info(f"POST #{self.request_count}: {url}")
            
            if json_data:
                response = self.session.post(url, json=json_data, timeout=timeout, **kwargs)
            else:
                response = self.session.post(url, data=data, timeout=timeout, **kwargs)
            
            response.raise_for_status()
            
            self.logger.info(f"POSTæˆåŠŸ {response.status_code}: {len(response.content)} å­—èŠ‚")
            
            self.smart_delay()
            
            return response
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"POSTå¤±è´¥ {url}: {e}")
            return None
    
    def smart_delay(self):
        """æ™ºèƒ½å»¶è¿Ÿ - æ ¹æ®è¯·æ±‚é¢‘ç‡è‡ªåŠ¨è°ƒæ•´"""
        base_delay = random.uniform(*self.delay_range)
        
        # æ ¹æ®è¯·æ±‚é¢‘ç‡è°ƒæ•´å»¶è¿Ÿ
        if self.request_count > 50:
            base_delay *= 1.5  # å¢åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
        elif self.request_count > 100:
            base_delay *= 2
        
        self.logger.debug(f"å»¶è¿Ÿ {base_delay:.2f} ç§’")
        time.sleep(base_delay)
    
    def get_json(self, url, **kwargs):
        """è·å–JSONæ•°æ®"""
        response = self.get_page(url, **kwargs)
        if response:
            try:
                return response.json()
            except json.JSONDecodeError as e:
                self.logger.error(f"JSONè§£æå¤±è´¥ {url}: {e}")
        return None
    
    def download_file(self, url, filename, chunk_size=8192):
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            self.logger.info(f"ä¸‹è½½æ–‡ä»¶: {url} -> {filename}")
            
            response = self.session.get(url, stream=True)
            response.raise_for_status()
            
            with open(filename, 'wb') as f:
                for chunk in response.iter_content(chunk_size=chunk_size):
                    if chunk:
                        f.write(chunk)
            
            self.logger.info(f"æ–‡ä»¶ä¸‹è½½å®Œæˆ: {filename}")
            self.smart_delay()
            
            return True
            
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def crawl_sitemap(self, sitemap_url):
        """çˆ¬å–ç½‘ç«™åœ°å›¾"""
        self.logger.info(f"çˆ¬å–ç½‘ç«™åœ°å›¾: {sitemap_url}")
        
        response = self.get_page(sitemap_url)
        if not response:
            return []
        
        # ç®€å•çš„sitemapè§£æ (å®é™…é¡¹ç›®ä¸­å»ºè®®ä½¿ç”¨ä¸“é—¨çš„XMLè§£æå™¨)
        urls = []
        import re
        url_pattern = r'<loc>(.*?)</loc>'
        urls = re.findall(url_pattern, response.text)
        
        self.logger.info(f"å‘ç° {len(urls)} ä¸ªURL")
        return urls
    
    def check_proxy_status(self):
        """æ£€æŸ¥ä»£ç†çŠ¶æ€"""
        self.logger.info("æ£€æŸ¥ä»£ç†çŠ¶æ€...")
        
        test_urls = [
            'http://httpbin.org/ip',
            'https://api.ipify.org?format=json',
            'http://ip-api.com/json'
        ]
        
        for url in test_urls:
            data = self.get_json(url)
            if data:
                self.logger.info(f"ä»£ç†IPæ£€æµ‹ {url}: {data}")
            else:
                self.logger.warning(f"ä»£ç†æ£€æµ‹å¤±è´¥: {url}")

def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    print("ğŸš€ å¯åŠ¨ç”Ÿäº§çº§ä»£ç†çˆ¬è™«...")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = ProductionCrawler(delay_range=(1, 3))
    
    # æ£€æŸ¥ä»£ç†çŠ¶æ€
    crawler.check_proxy_status()
    
    print("\n" + "="*60)
    print("ğŸ•·ï¸ çˆ¬è™«ç¤ºä¾‹")
    print("="*60)
    
    # ç¤ºä¾‹1: çˆ¬å–JSON API
    print("\n1. æµ‹è¯•JSON API...")
    data = crawler.get_json('https://jsonplaceholder.typicode.com/posts/1')
    if data:
        print(f"âœ… è·å–æ•°æ®: {data.get('title', 'N/A')}")
    
    # ç¤ºä¾‹2: çˆ¬å–HTMLé¡µé¢
    print("\n2. æµ‹è¯•HTMLé¡µé¢...")
    response = crawler.get_page('https://httpbin.org/html')
    if response:
        print(f"âœ… è·å–HTML: {len(response.text)} å­—ç¬¦")
    
    # ç¤ºä¾‹3: æµ‹è¯•POSTè¯·æ±‚
    print("\n3. æµ‹è¯•POSTè¯·æ±‚...")
    response = crawler.post_data(
        'http://httpbin.org/post',
        json_data={'message': 'Hello from proxy crawler!'}
    )
    if response:
        result = response.json()
        print(f"âœ… POSTæˆåŠŸ: {result.get('json', {})}")
    
    # ç¤ºä¾‹4: æµ‹è¯•çœŸå®ç½‘ç«™
    print("\n4. æµ‹è¯•çœŸå®ç½‘ç«™...")
    response = crawler.get_page('https://www.baidu.com')
    if response:
        print(f"âœ… ç™¾åº¦é¦–é¡µ: {response.status_code}")
    
    print(f"\nğŸ“Š æ€»è¯·æ±‚æ•°: {crawler.request_count}")
    print("ğŸ‰ çˆ¬è™«æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    example_usage()